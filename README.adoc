= ORKG Backend Prototype
Manuel Prinz <manuel.prinz@tib.eu>

:icons: font

This repository contains a backend prototype for the ORKG based on the https://spring.io/[Spring Framework].
It is written in https://kotlinlang.org/[Kotlin] as a proof-of-concept and for experimenting with a possible architecture and technologies.

== Building and running

=== Prerequisites

The prototype can be run stand-alone.
However, it needs other services in order to start properly.
Services are managed via https://www.docker.com/community-edition[Docker (CE)] and https://docs.docker.com/compose/[Docker Compose].
Please follow the respective installation instructions for your operating system or distribution.

=== Stand-alone application

NOTE: Running the application stand-alone is mostly for development purposes.
      It still needs Neo4j configured and running.
      If you want to test the application, please refer to the section <<Building a Docker image>> below.
      Alternatively, build the necessary images yourself by following the advice in the section <<Docker images>> (not recommended).

To build and run the prototype, type:

    ./gradlew bootRun

This will start a server running on http://localhost:8000.

The REST entry-point is http://localhost:8000/api/.

This also requires a Blazegraph server running on http://localhost:8889/bigdata/.
It expects the SumSur ontology to be loaded into the triple-store.
See the instructions below.

=== Building a Docker image

NOTE: The following commands assume that you added your user to the `docker` group https://docs.docker.com/install/linux/linux-postinstall/[as suggested in the documentation].
      If you did not do so, you have to prefix all commands with `sudo`.

The application can also run as a Docker container.
To build the image(s), run:

    ./gradlew docker

This will build the application and create the Docker image.
The application can be run via https://docs.docker.com/compose/[Docker Compose].
After installing it, run:

    docker-compose up -d

This will start the application image and all dependent containers.
It will take care of linking all containers so the services can see each other.
You can start and connect all services yourself but this is not recommended.

The application can be accessed via http://localhost:8000/.
The other services can be accessed via the URLs described in the table "<<endpoints>>".

To diagnose problems, check the logs with:

    docker-compose logs -f

WARNING: The following steps will destroy all data you saved to the database!

To restart from scratch, run:

    docker-compose stop
    docker-compose rm

== Docker images

NOTE: This section is only relevant if you want to work with some of the components in isolation.
      Please refer to the section <<Building a Docker image>> if you want to run the application via Docker.

=== Building the images

To build all Docker images (that contain the service dependencies), run:

    ./gradlew docker

If you want to build only specific images, either

[loweralpha]
. call `./gradlew :docker/IMAGE:docker` from the project root directory, or
. go to a `docker/IMAGE` directory and call `../../gradlew docker` there,

where `IMAGE` is the name of a sub-directory under `docker/`.

=== Running the images

The easiest way to start Docker containers from the images is via https://docs.docker.com/compose/[Docker Compose].
After installing it, run:

    docker-compose -f docker/IMAGE/docker-compose.yml up -d

where `IMAGE` is the name of a sub-directory under `docker/`.

=== Image end-points

The images will provide the following end-points:

.Images and their default end-points
[[endpoints]]
[cols=3*,options=header]
|===
|Image
|Container name
|End-points

|`orkg/blazegraph-dev`
|`orkgprototype_blazegraph_1`
|http://localhost:8889/bigdata/

|`orkg/neo4j-dev`
|`orkgprototype_neo4j_1`
| http://localhost:7474/browser/ +
bolt://localhost:7687

|===

=== Uploading data

==== Uploading to Blazegraph

In order to be useful, you need to import data into Blazegraph.
This has to be done manually.
Download the file `SemSur_3-labels.owl` from the https://git.tib.eu/orkg/orkg-ontology[`orkg-ontology` repository] and upload it to Blazegraph using the http://localhost:8889/bigdata/#update["Update" tab].

==== Uploading to Neo4j

The following assumes that a directory exists in the container
as `/data` and contains a file named `SemSur_3.owl`.
If the container is started via Docker Compose, a volume is
created and mounted for you automatically.
To copy the ontology into that volume, run:

  docker cp ~/Downloads/SemSur_3.owl neo4j_neo4j_1:/data

NOTE: As long as the volume is not deleted manually, the
      file will still be present even after re-creating the
      container.

To upload the SemSur data into Neo4j with the `semantics`
extension several steps need to be performed:

1. In the Neo4j web interface, create an index on the `uri` property of
   nodes with the `Resource` label:

  CREATE INDEX ON :Resource(uri)

2. Neo4j is used with RDF prefixes. These need to be communicated to the
   `semantics` extension via a special node. You can create the Cypher
   query to create it from your ontology file by using the helper script
   `scripts/NamespacesToNeoSemantics.kts`.footnote:[This needs
     https://github.com/holgerbrandl/kscript[kscript] to be installed.
     This can be done easily by http://sdkman.io/[SDKMAN].]
   Copy the output to the Neo4j web interface.

3. Import the ontology (data) via the Neo4j web interface:

  CALL semantics.importRDF("file:///data/SemSur_3.owl", "RDF/XML", { shortenUrls: true })

===== If something goes wrongâ€¦

WARNING: The following will destroy all your data!

If bad things and you want to reset the database, you can
either re-create the Docker container or execute the following
Cypher query:

  MATCH (n) DETACH DELETE n

== Features

Not much, to be honest. Particularly,

* a single REST resource, and
* a repository that queries a Blazegraph server using SPARQL.

An implementation of the `ArticleRepository` for Blazegraph is done.
One for Neo4j will follow.
(The idea is to explore how to implement vendor-independent data models and a clean architecture.)
